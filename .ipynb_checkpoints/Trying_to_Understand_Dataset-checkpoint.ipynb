{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data. Dataset\n",
    "\n",
    "This is a trivial exersice to understand `tf.data.Dataset`\n",
    "\n",
    "This API supports writing descriptive and efficient input pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating numpy array of integer numbers.\n",
    "\n",
    "This is source or input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting input dataset into tensorflow Dataset object, by slicing it using `from_tensor_slices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = tf.data.Dataset.from_tensor_slices(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one shot iterator\n",
    "Next step is to create an Iterator for elements of dataset using `make_one_shot_iterator`. \n",
    "\n",
    "The iterator arising from this method can only be initialized and run once – it can’t be re-initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-32ac85699639>:1: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "iterator = input_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the iterator is created, next step is to setup a TensorFlow operation which extracts elements from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-order to visualise how data is extracted, we can run it in session.\n",
    "\n",
    "If eager execution is enabled, then we can directly print 'next_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        print(sess.run(next_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch\n",
    "\n",
    "Now lets explore `batch` function of Dataset, \n",
    "\n",
    "`batch()` - is function in `tf.data.Dataset` which creates batch based on `batch count`\n",
    "\n",
    "Here we took batch count=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Done\n",
      "[2 3]\n",
      "Done\n",
      "[4 5]\n",
      "Done\n",
      "[6 7]\n",
      "Done\n",
      "[8 9]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "input_dataset_batch2 = input_dataset.batch(2)\n",
    "\n",
    "iterator = input_dataset_batch2.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "            print(\"Done\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_batch = input_dataset.batch(3)\n",
    "\n",
    "iterator = input_dataset_batch.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "\n",
    "This function shuffles dataset, we need to specify buffersize.\n",
    "\n",
    "Observe the dataset sequence in below examples, \n",
    "\n",
    "`shuffle()` after `batch()` shuffles batches.\n",
    "\n",
    "`shuffle()` before `batch()` shuffles dataset and the creates batch.\n",
    "\n",
    "`buffersize` = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[0 1]\n",
      "[8 9]\n",
      "[6 7]\n",
      "[4 5]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_batch_shuffle = input_dataset.batch(2).shuffle(10)\n",
    "\n",
    "iterator = input_dataset_batch_shuffle.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 6]\n",
      "[0 2]\n",
      "[4 3]\n",
      "[9 7]\n",
      "[5 1]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_shuffle_batch = input_dataset.shuffle(10).batch(2)\n",
    "\n",
    "iterator = input_dataset_shuffle_batch.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that using `shuffle` after `batch` will just shuffle batches not data,\n",
    "Best way is to `shuffle` and `batch`, so that data is shufled across batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip\n",
    "If we have multiple dataset to be combined, or need to combine train value and train label, then we use `zip`\n",
    "\n",
    "`zip` is function which can zip or combine different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([0, 1, 2]))\n",
      "(array([3, 4, 5]), array([3, 4, 5]))\n",
      "(array([6, 7, 8]), array([6, 7, 8]))\n",
      "(array([9]), array([9]))\n"
     ]
    }
   ],
   "source": [
    "combined_dataset = tf.data.Dataset.zip((input_dataset, input_dataset)).batch(3)\n",
    "\n",
    "iterator = combined_dataset.make_one_shot_iterator()\n",
    "next_data =iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()\n",
    "If any function is to be applied to dataset, then `map` is used\n",
    "\n",
    "`map` is a function which applies transformation to each elements of input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "input_dataset_fnc = input_dataset.map(lambda x: x+2)\n",
    "\n",
    "iterator = input_dataset_fnc.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping another function to input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "input_dataset_fnc_2 = input_dataset.map(lambda x: x+2, num_parallel_calls=-1)\n",
    "\n",
    "iterator = input_dataset_fnc_2.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat()\n",
    "\n",
    "Now we explore `repeat()` function of `tf.data.Dataset`\n",
    "\n",
    "`repeat` function has argument `count`, which asks how many time to repeat the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** : when `repeat()` is used without any argument, then the code is executed indefinitely without throwing OutOfRangeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n",
      "[5 6 7]\n",
      "[ 8  9 10]\n",
      "[11  2  3]\n",
      "[4 5 6]\n",
      "[7 8 9]\n",
      "[10 11  2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[ 9 10 11]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_fnc_3 = input_dataset.map(lambda x: x+2).repeat(count=3).batch(3)\n",
    "\n",
    "iterator = input_dataset_fnc_3.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one hot encoding\n",
    "\n",
    "Inorder to convert label to one hot, we can apply tensorflow inbuilt function `tf.one_hot` to entire dataset using `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_hot = input_dataset.map(lambda x: tf.one_hot(x,10)).batch(2)\n",
    "\n",
    "iterator = input_dataset_hot.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from <https://www.tensorflow.org/guide/performance/datasets#map_and_interleave_prefetch_shuffle>\n",
    "\n",
    "The tf.data.Dataset.repeat transformation repeats the input data a finite (or infinite) number of times; each repetition of the data is typically referred to as an epoch. The tf.data.Dataset.shuffle transformation randomizes the order of the dataset's examples.\n",
    "\n",
    "If the repeat transformation is applied before the shuffle transformation, then the epoch boundaries are blurred. That is, certain elements can be repeated before other elements appear even once. On the other hand, if the shuffle transformation is applied before the repeat transformation, then performance might slow down at the beginning of each epoch related to initialization of the internal state of the shuffle transformation. In other words, the former (repeat before shuffle) provides better performance, while the latter (shuffle before repeat) provides stronger ordering guarantees.\n",
    "\n",
    "\"We recommend applying the shuffle transformation before the repeat transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us observe few combination of these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0]\n",
      "[1 5]\n",
      "[3 6]\n",
      "[7 8]\n",
      "[2 9]\n",
      "[3 2]\n",
      "[6 5]\n",
      "[4 8]\n",
      "[7 9]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_s_b_r = input_dataset.shuffle(5).batch(2).repeat(2)\n",
    "\n",
    "iterator = input_dataset_s_b_r.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4]\n",
      "[0 3]\n",
      "[8 6]\n",
      "[2 5]\n",
      "[9 7]\n",
      "[3 4]\n",
      "[0 6]\n",
      "[8 9]\n",
      "[5 7]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_s_r_b = input_dataset.shuffle(5).repeat(2).batch(2)\n",
    "\n",
    "iterator = input_dataset_s_r_b.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5]\n",
      "[6 7]\n",
      "[4 0]\n",
      "[3 0]\n",
      "[8 1]\n",
      "[4 2]\n",
      "[3 5]\n",
      "[2 9]\n",
      "[7 6]\n",
      "[9 8]\n"
     ]
    }
   ],
   "source": [
    "input_dataset_r_s_b = input_dataset.repeat(2).shuffle(5).batch(2)\n",
    "\n",
    "iterator = input_dataset_r_s_b.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_data))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "From above example we can makeout that (consider epoch as one set of batch (0,1) (2,3)...))\n",
    "\n",
    "1) when `shuffle` and `repeat`, the data is shuffled within epoch, i,e there exist clear boundary\n",
    "\n",
    "2) when `repeat` and `shuffle`, there is no proper boundary, you have duplicate data in same epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eloberate or access each elements in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0\n",
      "1\n",
      "[2 3]\n",
      "2\n",
      "3\n",
      "[4 5]\n",
      "4\n",
      "5\n",
      "[6 7]\n",
      "6\n",
      "7\n",
      "[8 9]\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ex1 = input_dataset.batch(2)\n",
    "iterator = ex1.make_one_shot_iterator()\n",
    "next_image = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        num = sess.run(next_image)\n",
    "        print(num)\n",
    "        print(num[0])\n",
    "        print(num[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flat_map\n",
    "\n",
    "Use `flat_map` if you want to make sure that the order of your dataset stays the same. \n",
    "\n",
    "For example, to flatten a dataset of batches into a dataset of their elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "ary_2 = np.arange(0,12)\n",
    "a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = a.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x+1))\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interleave\n",
    "\n",
    "`interleave` maps function across dataset, \n",
    "\n",
    "It parallely access data from different batches of dataset using `cycle length`.\n",
    "\n",
    "Below is the example with different values of `cycle lenght`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with just batch size of 2\n",
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n",
      "[8 9]\n",
      "[10 11]\n",
      "This is with batch size of 2, cycle length 2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "7\n",
      "8\n",
      "10\n",
      "9\n",
      "11\n",
      "This is with batch size of 2, cycle length 4\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "8\n",
      "10\n",
      "9\n",
      "11\n",
      "This is with batch size of 2, cycle length 6\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "This implies cycle_length controls the number of input elements that are processed concurrently.\n"
     ]
    }
   ],
   "source": [
    "ary_2 = np.arange(0,12)\n",
    "a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "# a = a.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x+1))\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with just batch size of 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "ary_3 = tf.data.Dataset.range(0,12).batch(2)\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=2)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=4)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 4\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=6)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 6\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"This implies cycle_length controls the number of input elements that are processed concurrently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`block_lenght`\n",
    "\n",
    "If we want to access more than one element in each batch in interleave maping then we use `block_lenght`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with just batch size of 2\n",
      "[0 1 2 3]\n",
      "[4 5 6 7]\n",
      "[ 8  9 10 11]\n",
      "This is with batch size of 2, cycle length 2, block_length 1\n",
      "0\n",
      "4\n",
      "1\n",
      "5\n",
      "2\n",
      "6\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "This is with batch size of 2, cycle length 2, block_length 2\n",
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "This is with batch size of 2, cycle length 2, block_length 3\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "This implies block_length controls the number of       consecutive elements to produce from each input element before cycling to another input element..\n"
     ]
    }
   ],
   "source": [
    "ary_2 = np.arange(0,12)\n",
    "a = tf.data.Dataset.from_tensor_slices(ary_2).batch(4)\n",
    "# a = a.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x+1))\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with just batch size of 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "ary_3 = tf.data.Dataset.range(0,12).batch(4)\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=2, block_length = 1)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 1\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=2, block_length = 2)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# a = tf.data.Dataset.from_tensor_slices(ary_2).batch(2)\n",
    "a = ary_3.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "                                                     cycle_length=2, block_length = 3)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 3\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"This implies block_length controls the number of \\\n",
    "      consecutive elements to produce from each input element before cycling to another input element..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few miscellaneous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with batch size of 6\n",
      "[0 1 2 3 4 5]\n",
      "[ 6  7  8  9 10 11]\n",
      "This is with batch size of 2, cycle length 2, block_length 1, shuffle 2\n",
      "0\n",
      "1\n",
      "7\n",
      "2\n",
      "8\n",
      "3\n",
      "6\n",
      "4\n",
      "10\n",
      "5\n",
      "9\n",
      "11\n",
      "This is with batch size of 2, cycle length 2, block_length 1, shuffle 1\n",
      "0\n",
      "6\n",
      "1\n",
      "7\n",
      "2\n",
      "8\n",
      "3\n",
      "9\n",
      "4\n",
      "10\n",
      "5\n",
      "11\n",
      "This is with batch size of 2, cycle length 2, block_length 2, shuffle 4\n",
      "0\n",
      "DONE...\n",
      "6\n",
      "DONE...\n",
      "8\n",
      "DONE...\n",
      "1\n",
      "DONE...\n",
      "9\n",
      "DONE...\n",
      "3\n",
      "DONE...\n",
      "2\n",
      "DONE...\n",
      "5\n",
      "DONE...\n",
      "7\n",
      "DONE...\n",
      "4\n",
      "DONE...\n",
      "10\n",
      "DONE...\n",
      "11\n",
      "DONE...\n"
     ]
    }
   ],
   "source": [
    "def parse_fn(x):\n",
    "    return x\n",
    "\n",
    "ary_4 = tf.data.Dataset.range(0,12).batch(6)\n",
    "\n",
    "iterator = ary_4.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 6\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "a = ary_4.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x).map(parse_fn),\n",
    "                                                     cycle_length=2, block_length = 1).shuffle(2)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 1, shuffle 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "a = ary_4.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x).map(parse_fn),\n",
    "                                                     cycle_length=2, block_length = 1).shuffle(1)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 1, shuffle 1\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "a = ary_4.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x).map(parse_fn),\n",
    "                                                     cycle_length=2, block_length = 1).shuffle(4)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 2, shuffle 4\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "            print(\"DONE...\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with batch size of 6\n",
      "[0 1 2 3 4 5]\n",
      "[ 6  7  8  9 10 11]\n",
      "This is with batch size of 2, cycle length 2, block_length 1, shuffle 2\n",
      "[6 1]\n",
      "[0 7]\n",
      "[8 3]\n",
      "[9 4]\n",
      "[2 5]\n",
      "[10 11]\n"
     ]
    }
   ],
   "source": [
    "def parse_fn(x):\n",
    "    return x\n",
    "\n",
    "ary_4 = tf.data.Dataset.range(0,12).batch(6)\n",
    "\n",
    "iterator = ary_4.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 6\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "a = ary_4.interleave(lambda x: tf.data.Dataset.from_tensor_slices(x).map(parse_fn),\n",
    "                                                     cycle_length=2, block_length = 1).shuffle(2).batch(2)\n",
    "\n",
    "\n",
    "iterator = a.make_one_shot_iterator()\n",
    "next_one = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"This is with batch size of 2, cycle length 2, block_length 1, shuffle 2\")\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_one))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
